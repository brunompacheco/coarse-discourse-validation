{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bruno/Desktop/coarse-discourse-validation\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "PROJ_ROOT = os.path.abspath(os.path.join(os.pardir))\n",
    "print(PROJ_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/bruno/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotations</th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>id</th>\n",
       "      <th>in_reply_to</th>\n",
       "      <th>is_self_post</th>\n",
       "      <th>majority_link</th>\n",
       "      <th>majority_type</th>\n",
       "      <th>post_depth</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>thread_author</th>\n",
       "      <th>thread_title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'link_to_post': 'none', 'main_type': 'announ...</td>\n",
       "      <td>DTX120</td>\n",
       "      <td>4/7/13  \\n\\n7/27/12  \\n\\nhttp://www.imdb.com/t...</td>\n",
       "      <td>t3_1bx6qw</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>announcement</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100movies365days</td>\n",
       "      <td>DTX120</td>\n",
       "      <td>DTX120: #87 - Nashville</td>\n",
       "      <td>https://www.reddit.com/r/100movies365days/comm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'link_to_post': 't3_1bx6qw', 'main_type': 'a...</td>\n",
       "      <td>mcgrewf10</td>\n",
       "      <td>I've wanted to watch this for a long time. I w...</td>\n",
       "      <td>t1_c9b2nyd</td>\n",
       "      <td>t3_1bx6qw</td>\n",
       "      <td>True</td>\n",
       "      <td>t3_1bx6qw</td>\n",
       "      <td>elaboration</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100movies365days</td>\n",
       "      <td>DTX120</td>\n",
       "      <td>DTX120: #87 - Nashville</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{'link_to_post': 't1_c9b2nyd', 'main_type': '...</td>\n",
       "      <td>DTX120</td>\n",
       "      <td>You strike me as the type who would appreciate...</td>\n",
       "      <td>t1_c9b30i1</td>\n",
       "      <td>t1_c9b2nyd</td>\n",
       "      <td>True</td>\n",
       "      <td>t1_c9b2nyd</td>\n",
       "      <td>elaboration</td>\n",
       "      <td>2.0</td>\n",
       "      <td>100movies365days</td>\n",
       "      <td>DTX120</td>\n",
       "      <td>DTX120: #87 - Nashville</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{'link_to_post': 't1_c9b30i1', 'main_type': '...</td>\n",
       "      <td>mcgrewf10</td>\n",
       "      <td>Yeah, I've always heard that Altman was famous...</td>\n",
       "      <td>t1_c9b6sj0</td>\n",
       "      <td>t1_c9b30i1</td>\n",
       "      <td>True</td>\n",
       "      <td>t1_c9b30i1</td>\n",
       "      <td>elaboration</td>\n",
       "      <td>3.0</td>\n",
       "      <td>100movies365days</td>\n",
       "      <td>DTX120</td>\n",
       "      <td>DTX120: #87 - Nashville</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{'link_to_post': 'none', 'main_type': 'announ...</td>\n",
       "      <td>Keatonus</td>\n",
       "      <td>Alright guys, little background about myself. ...</td>\n",
       "      <td>t3_omv7p</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>announcement</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100sets</td>\n",
       "      <td>Keatonus</td>\n",
       "      <td>Male, 23 years old. Going for 100 sets!</td>\n",
       "      <td>https://www.reddit.com/r/100sets/comments/omv7...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         annotations     author  \\\n",
       "0  [{'link_to_post': 'none', 'main_type': 'announ...     DTX120   \n",
       "1  [{'link_to_post': 't3_1bx6qw', 'main_type': 'a...  mcgrewf10   \n",
       "2  [{'link_to_post': 't1_c9b2nyd', 'main_type': '...     DTX120   \n",
       "3  [{'link_to_post': 't1_c9b30i1', 'main_type': '...  mcgrewf10   \n",
       "4  [{'link_to_post': 'none', 'main_type': 'announ...   Keatonus   \n",
       "\n",
       "                                                body          id in_reply_to  \\\n",
       "0  4/7/13  \\n\\n7/27/12  \\n\\nhttp://www.imdb.com/t...   t3_1bx6qw         NaN   \n",
       "1  I've wanted to watch this for a long time. I w...  t1_c9b2nyd   t3_1bx6qw   \n",
       "2  You strike me as the type who would appreciate...  t1_c9b30i1  t1_c9b2nyd   \n",
       "3  Yeah, I've always heard that Altman was famous...  t1_c9b6sj0  t1_c9b30i1   \n",
       "4  Alright guys, little background about myself. ...    t3_omv7p         NaN   \n",
       "\n",
       "   is_self_post majority_link majority_type  post_depth         subreddit  \\\n",
       "0          True          none  announcement         0.0  100movies365days   \n",
       "1          True     t3_1bx6qw   elaboration         1.0  100movies365days   \n",
       "2          True    t1_c9b2nyd   elaboration         2.0  100movies365days   \n",
       "3          True    t1_c9b30i1   elaboration         3.0  100movies365days   \n",
       "4          True          none  announcement         0.0           100sets   \n",
       "\n",
       "  thread_author                             thread_title  \\\n",
       "0        DTX120                  DTX120: #87 - Nashville   \n",
       "1        DTX120                  DTX120: #87 - Nashville   \n",
       "2        DTX120                  DTX120: #87 - Nashville   \n",
       "3        DTX120                  DTX120: #87 - Nashville   \n",
       "4      Keatonus  Male, 23 years old. Going for 100 sets!   \n",
       "\n",
       "                                                 url  \n",
       "0  https://www.reddit.com/r/100movies365days/comm...  \n",
       "1                                                NaN  \n",
       "2                                                NaN  \n",
       "3                                                NaN  \n",
       "4  https://www.reddit.com/r/100sets/comments/omv7...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_path = os.path.join(PROJ_ROOT, 'data', 'interim', 'clean_data.pkl')\n",
    "\n",
    "df = pd.read_pickle(df_path)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotations</th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>id</th>\n",
       "      <th>in_reply_to</th>\n",
       "      <th>is_self_post</th>\n",
       "      <th>majority_link</th>\n",
       "      <th>majority_type</th>\n",
       "      <th>post_depth</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>thread_author</th>\n",
       "      <th>thread_title</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'link_to_post': 'none', 'main_type': 'announ...</td>\n",
       "      <td>DTX120</td>\n",
       "      <td>4/7/13  \\n\\n7/27/12  \\n\\nhttp://www.imdb.com/t...</td>\n",
       "      <td>t3_1bx6qw</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>announcement</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100movies365days</td>\n",
       "      <td>DTX120</td>\n",
       "      <td>DTX120: #87 - Nashville</td>\n",
       "      <td>https://www.reddit.com/r/100movies365days/comm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'link_to_post': 't3_1bx6qw', 'main_type': 'a...</td>\n",
       "      <td>mcgrewf10</td>\n",
       "      <td>I've wanted to watch this for a long time. I w...</td>\n",
       "      <td>t1_c9b2nyd</td>\n",
       "      <td>t3_1bx6qw</td>\n",
       "      <td>True</td>\n",
       "      <td>t3_1bx6qw</td>\n",
       "      <td>elaboration</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100movies365days</td>\n",
       "      <td>DTX120</td>\n",
       "      <td>DTX120: #87 - Nashville</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{'link_to_post': 't1_c9b2nyd', 'main_type': '...</td>\n",
       "      <td>DTX120</td>\n",
       "      <td>You strike me as the type who would appreciate...</td>\n",
       "      <td>t1_c9b30i1</td>\n",
       "      <td>t1_c9b2nyd</td>\n",
       "      <td>True</td>\n",
       "      <td>t1_c9b2nyd</td>\n",
       "      <td>elaboration</td>\n",
       "      <td>2.0</td>\n",
       "      <td>100movies365days</td>\n",
       "      <td>DTX120</td>\n",
       "      <td>DTX120: #87 - Nashville</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{'link_to_post': 't1_c9b30i1', 'main_type': '...</td>\n",
       "      <td>mcgrewf10</td>\n",
       "      <td>Yeah, I've always heard that Altman was famous...</td>\n",
       "      <td>t1_c9b6sj0</td>\n",
       "      <td>t1_c9b30i1</td>\n",
       "      <td>True</td>\n",
       "      <td>t1_c9b30i1</td>\n",
       "      <td>elaboration</td>\n",
       "      <td>3.0</td>\n",
       "      <td>100movies365days</td>\n",
       "      <td>DTX120</td>\n",
       "      <td>DTX120: #87 - Nashville</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{'link_to_post': 'none', 'main_type': 'announ...</td>\n",
       "      <td>Keatonus</td>\n",
       "      <td>Alright guys, little background about myself. ...</td>\n",
       "      <td>t3_omv7p</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>announcement</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100sets</td>\n",
       "      <td>Keatonus</td>\n",
       "      <td>Male, 23 years old. Going for 100 sets!</td>\n",
       "      <td>https://www.reddit.com/r/100sets/comments/omv7...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         annotations     author  \\\n",
       "0  [{'link_to_post': 'none', 'main_type': 'announ...     DTX120   \n",
       "1  [{'link_to_post': 't3_1bx6qw', 'main_type': 'a...  mcgrewf10   \n",
       "2  [{'link_to_post': 't1_c9b2nyd', 'main_type': '...     DTX120   \n",
       "3  [{'link_to_post': 't1_c9b30i1', 'main_type': '...  mcgrewf10   \n",
       "4  [{'link_to_post': 'none', 'main_type': 'announ...   Keatonus   \n",
       "\n",
       "                                                body          id in_reply_to  \\\n",
       "0  4/7/13  \\n\\n7/27/12  \\n\\nhttp://www.imdb.com/t...   t3_1bx6qw         NaN   \n",
       "1  I've wanted to watch this for a long time. I w...  t1_c9b2nyd   t3_1bx6qw   \n",
       "2  You strike me as the type who would appreciate...  t1_c9b30i1  t1_c9b2nyd   \n",
       "3  Yeah, I've always heard that Altman was famous...  t1_c9b6sj0  t1_c9b30i1   \n",
       "4  Alright guys, little background about myself. ...    t3_omv7p         NaN   \n",
       "\n",
       "   is_self_post majority_link majority_type  post_depth         subreddit  \\\n",
       "0          True          none  announcement         0.0  100movies365days   \n",
       "1          True     t3_1bx6qw   elaboration         1.0  100movies365days   \n",
       "2          True    t1_c9b2nyd   elaboration         2.0  100movies365days   \n",
       "3          True    t1_c9b30i1   elaboration         3.0  100movies365days   \n",
       "4          True          none  announcement         0.0           100sets   \n",
       "\n",
       "  thread_author                             thread_title  \\\n",
       "0        DTX120                  DTX120: #87 - Nashville   \n",
       "1        DTX120                  DTX120: #87 - Nashville   \n",
       "2        DTX120                  DTX120: #87 - Nashville   \n",
       "3        DTX120                  DTX120: #87 - Nashville   \n",
       "4      Keatonus  Male, 23 years old. Going for 100 sets!   \n",
       "\n",
       "                                                 url  \n",
       "0  https://www.reddit.com/r/100movies365days/comm...  \n",
       "1                                                NaN  \n",
       "2                                                NaN  \n",
       "3                                                NaN  \n",
       "4  https://www.reddit.com/r/100sets/comments/omv7...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample = df.iloc[0:1000]\n",
    "\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features Exploration\n",
    "\n",
    "In here, we'll be working on the features described on the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure\n",
    "\n",
    "> We calculate several features related to the structure of the comment and its position. One feature is the depth of the comment according to Redditâ€™s threaded structure, which we collect as both a raw count and normalized by the number of comments in the discussion. We also calculate number of sentences, number of words, and number of characters of both the body and the title of the comment. We computed these values for both the current comment and the parent comment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0     412\n",
       "2.0     210\n",
       "3.0     129\n",
       "0.0     107\n",
       "4.0      73\n",
       "5.0      31\n",
       "6.0      15\n",
       "7.0       8\n",
       "8.0       6\n",
       "10.0      5\n",
       "9.0       4\n",
       "Name: post_depth, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample['post_depth'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sample['post_depth_normalized'] = df_sample['post_depth'] / df_sample['comments_in_discussion']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of sentences\n",
    "\n",
    "We use NLTK.tokenize to get setences tokenized and then counted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You won't regret keeping it. As far as longevity goes, it is almost indefinite. You will wear out parts like springs, mags, and maybe even the grips. The slide, frame, and barrel will likely outlive you. Colt makes great guns. How many companies who produced pistols 100 years ago still have their products found on gun shop shelves in working and safe condition?\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample['body'].iloc[42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/bruno/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"You won't regret keeping it.\",\n",
       " 'As far as longevity goes, it is almost indefinite.',\n",
       " 'You will wear out parts like springs, mags, and maybe even the grips.',\n",
       " 'The slide, frame, and barrel will likely outlive you.',\n",
       " 'Colt makes great guns.',\n",
       " 'How many companies who produced pistols 100 years ago still have their products found on gun shop shelves in working and safe condition?']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "\n",
    "nltk.tokenize.sent_tokenize(df_sample['body'].iloc[42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bruno/anaconda3/envs/coarse-discourse-validation/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotations</th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>id</th>\n",
       "      <th>in_reply_to</th>\n",
       "      <th>is_self_post</th>\n",
       "      <th>majority_link</th>\n",
       "      <th>majority_type</th>\n",
       "      <th>post_depth</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>thread_author</th>\n",
       "      <th>thread_title</th>\n",
       "      <th>url</th>\n",
       "      <th>n_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'link_to_post': 'none', 'main_type': 'announ...</td>\n",
       "      <td>DTX120</td>\n",
       "      <td>4/7/13  \\n\\n7/27/12  \\n\\nhttp://www.imdb.com/t...</td>\n",
       "      <td>t3_1bx6qw</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>announcement</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100movies365days</td>\n",
       "      <td>DTX120</td>\n",
       "      <td>DTX120: #87 - Nashville</td>\n",
       "      <td>https://www.reddit.com/r/100movies365days/comm...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'link_to_post': 't3_1bx6qw', 'main_type': 'a...</td>\n",
       "      <td>mcgrewf10</td>\n",
       "      <td>I've wanted to watch this for a long time. I w...</td>\n",
       "      <td>t1_c9b2nyd</td>\n",
       "      <td>t3_1bx6qw</td>\n",
       "      <td>True</td>\n",
       "      <td>t3_1bx6qw</td>\n",
       "      <td>elaboration</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100movies365days</td>\n",
       "      <td>DTX120</td>\n",
       "      <td>DTX120: #87 - Nashville</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{'link_to_post': 't1_c9b2nyd', 'main_type': '...</td>\n",
       "      <td>DTX120</td>\n",
       "      <td>You strike me as the type who would appreciate...</td>\n",
       "      <td>t1_c9b30i1</td>\n",
       "      <td>t1_c9b2nyd</td>\n",
       "      <td>True</td>\n",
       "      <td>t1_c9b2nyd</td>\n",
       "      <td>elaboration</td>\n",
       "      <td>2.0</td>\n",
       "      <td>100movies365days</td>\n",
       "      <td>DTX120</td>\n",
       "      <td>DTX120: #87 - Nashville</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{'link_to_post': 't1_c9b30i1', 'main_type': '...</td>\n",
       "      <td>mcgrewf10</td>\n",
       "      <td>Yeah, I've always heard that Altman was famous...</td>\n",
       "      <td>t1_c9b6sj0</td>\n",
       "      <td>t1_c9b30i1</td>\n",
       "      <td>True</td>\n",
       "      <td>t1_c9b30i1</td>\n",
       "      <td>elaboration</td>\n",
       "      <td>3.0</td>\n",
       "      <td>100movies365days</td>\n",
       "      <td>DTX120</td>\n",
       "      <td>DTX120: #87 - Nashville</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{'link_to_post': 'none', 'main_type': 'announ...</td>\n",
       "      <td>Keatonus</td>\n",
       "      <td>Alright guys, little background about myself. ...</td>\n",
       "      <td>t3_omv7p</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>announcement</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100sets</td>\n",
       "      <td>Keatonus</td>\n",
       "      <td>Male, 23 years old. Going for 100 sets!</td>\n",
       "      <td>https://www.reddit.com/r/100sets/comments/omv7...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         annotations     author  \\\n",
       "0  [{'link_to_post': 'none', 'main_type': 'announ...     DTX120   \n",
       "1  [{'link_to_post': 't3_1bx6qw', 'main_type': 'a...  mcgrewf10   \n",
       "2  [{'link_to_post': 't1_c9b2nyd', 'main_type': '...     DTX120   \n",
       "3  [{'link_to_post': 't1_c9b30i1', 'main_type': '...  mcgrewf10   \n",
       "4  [{'link_to_post': 'none', 'main_type': 'announ...   Keatonus   \n",
       "\n",
       "                                                body          id in_reply_to  \\\n",
       "0  4/7/13  \\n\\n7/27/12  \\n\\nhttp://www.imdb.com/t...   t3_1bx6qw         NaN   \n",
       "1  I've wanted to watch this for a long time. I w...  t1_c9b2nyd   t3_1bx6qw   \n",
       "2  You strike me as the type who would appreciate...  t1_c9b30i1  t1_c9b2nyd   \n",
       "3  Yeah, I've always heard that Altman was famous...  t1_c9b6sj0  t1_c9b30i1   \n",
       "4  Alright guys, little background about myself. ...    t3_omv7p         NaN   \n",
       "\n",
       "   is_self_post majority_link majority_type  post_depth         subreddit  \\\n",
       "0          True          none  announcement         0.0  100movies365days   \n",
       "1          True     t3_1bx6qw   elaboration         1.0  100movies365days   \n",
       "2          True    t1_c9b2nyd   elaboration         2.0  100movies365days   \n",
       "3          True    t1_c9b30i1   elaboration         3.0  100movies365days   \n",
       "4          True          none  announcement         0.0           100sets   \n",
       "\n",
       "  thread_author                             thread_title  \\\n",
       "0        DTX120                  DTX120: #87 - Nashville   \n",
       "1        DTX120                  DTX120: #87 - Nashville   \n",
       "2        DTX120                  DTX120: #87 - Nashville   \n",
       "3        DTX120                  DTX120: #87 - Nashville   \n",
       "4      Keatonus  Male, 23 years old. Going for 100 sets!   \n",
       "\n",
       "                                                 url  n_sentences  \n",
       "0  https://www.reddit.com/r/100movies365days/comm...           29  \n",
       "1                                                NaN            2  \n",
       "2                                                NaN            4  \n",
       "3                                                NaN            2  \n",
       "4  https://www.reddit.com/r/100sets/comments/omv7...            5  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_number = lambda b : len(nltk.tokenize.sent_tokenize(b))\n",
    "\n",
    "df_sample['n_sentences'] = df_sample['body'].map(sent_number)\n",
    "\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['You',\n",
       " 'wo',\n",
       " \"n't\",\n",
       " 'regret',\n",
       " 'keeping',\n",
       " 'it',\n",
       " '.',\n",
       " 'As',\n",
       " 'far',\n",
       " 'as',\n",
       " 'longevity',\n",
       " 'goes',\n",
       " ',',\n",
       " 'it',\n",
       " 'is',\n",
       " 'almost',\n",
       " 'indefinite',\n",
       " '.',\n",
       " 'You',\n",
       " 'will',\n",
       " 'wear',\n",
       " 'out',\n",
       " 'parts',\n",
       " 'like',\n",
       " 'springs',\n",
       " ',',\n",
       " 'mags',\n",
       " ',',\n",
       " 'and',\n",
       " 'maybe',\n",
       " 'even',\n",
       " 'the',\n",
       " 'grips',\n",
       " '.',\n",
       " 'The',\n",
       " 'slide',\n",
       " ',',\n",
       " 'frame',\n",
       " ',',\n",
       " 'and',\n",
       " 'barrel',\n",
       " 'will',\n",
       " 'likely',\n",
       " 'outlive',\n",
       " 'you',\n",
       " '.',\n",
       " 'Colt',\n",
       " 'makes',\n",
       " 'great',\n",
       " 'guns',\n",
       " '.',\n",
       " 'How',\n",
       " 'many',\n",
       " 'companies',\n",
       " 'who',\n",
       " 'produced',\n",
       " 'pistols',\n",
       " '100',\n",
       " 'years',\n",
       " 'ago',\n",
       " 'still',\n",
       " 'have',\n",
       " 'their',\n",
       " 'products',\n",
       " 'found',\n",
       " 'on',\n",
       " 'gun',\n",
       " 'shop',\n",
       " 'shelves',\n",
       " 'in',\n",
       " 'working',\n",
       " 'and',\n",
       " 'safe',\n",
       " 'condition',\n",
       " '?']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.tokenize.word_tokenize(df_sample['body'].iloc[42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bruno/anaconda3/envs/coarse-discourse-validation/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotations</th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>id</th>\n",
       "      <th>in_reply_to</th>\n",
       "      <th>is_self_post</th>\n",
       "      <th>majority_link</th>\n",
       "      <th>majority_type</th>\n",
       "      <th>post_depth</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>thread_author</th>\n",
       "      <th>thread_title</th>\n",
       "      <th>url</th>\n",
       "      <th>n_sentences</th>\n",
       "      <th>n_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'link_to_post': 'none', 'main_type': 'announ...</td>\n",
       "      <td>DTX120</td>\n",
       "      <td>4/7/13  \\n\\n7/27/12  \\n\\nhttp://www.imdb.com/t...</td>\n",
       "      <td>t3_1bx6qw</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>announcement</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100movies365days</td>\n",
       "      <td>DTX120</td>\n",
       "      <td>DTX120: #87 - Nashville</td>\n",
       "      <td>https://www.reddit.com/r/100movies365days/comm...</td>\n",
       "      <td>29</td>\n",
       "      <td>499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'link_to_post': 't3_1bx6qw', 'main_type': 'a...</td>\n",
       "      <td>mcgrewf10</td>\n",
       "      <td>I've wanted to watch this for a long time. I w...</td>\n",
       "      <td>t1_c9b2nyd</td>\n",
       "      <td>t3_1bx6qw</td>\n",
       "      <td>True</td>\n",
       "      <td>t3_1bx6qw</td>\n",
       "      <td>elaboration</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100movies365days</td>\n",
       "      <td>DTX120</td>\n",
       "      <td>DTX120: #87 - Nashville</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{'link_to_post': 't1_c9b2nyd', 'main_type': '...</td>\n",
       "      <td>DTX120</td>\n",
       "      <td>You strike me as the type who would appreciate...</td>\n",
       "      <td>t1_c9b30i1</td>\n",
       "      <td>t1_c9b2nyd</td>\n",
       "      <td>True</td>\n",
       "      <td>t1_c9b2nyd</td>\n",
       "      <td>elaboration</td>\n",
       "      <td>2.0</td>\n",
       "      <td>100movies365days</td>\n",
       "      <td>DTX120</td>\n",
       "      <td>DTX120: #87 - Nashville</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{'link_to_post': 't1_c9b30i1', 'main_type': '...</td>\n",
       "      <td>mcgrewf10</td>\n",
       "      <td>Yeah, I've always heard that Altman was famous...</td>\n",
       "      <td>t1_c9b6sj0</td>\n",
       "      <td>t1_c9b30i1</td>\n",
       "      <td>True</td>\n",
       "      <td>t1_c9b30i1</td>\n",
       "      <td>elaboration</td>\n",
       "      <td>3.0</td>\n",
       "      <td>100movies365days</td>\n",
       "      <td>DTX120</td>\n",
       "      <td>DTX120: #87 - Nashville</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{'link_to_post': 'none', 'main_type': 'announ...</td>\n",
       "      <td>Keatonus</td>\n",
       "      <td>Alright guys, little background about myself. ...</td>\n",
       "      <td>t3_omv7p</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>announcement</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100sets</td>\n",
       "      <td>Keatonus</td>\n",
       "      <td>Male, 23 years old. Going for 100 sets!</td>\n",
       "      <td>https://www.reddit.com/r/100sets/comments/omv7...</td>\n",
       "      <td>5</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         annotations     author  \\\n",
       "0  [{'link_to_post': 'none', 'main_type': 'announ...     DTX120   \n",
       "1  [{'link_to_post': 't3_1bx6qw', 'main_type': 'a...  mcgrewf10   \n",
       "2  [{'link_to_post': 't1_c9b2nyd', 'main_type': '...     DTX120   \n",
       "3  [{'link_to_post': 't1_c9b30i1', 'main_type': '...  mcgrewf10   \n",
       "4  [{'link_to_post': 'none', 'main_type': 'announ...   Keatonus   \n",
       "\n",
       "                                                body          id in_reply_to  \\\n",
       "0  4/7/13  \\n\\n7/27/12  \\n\\nhttp://www.imdb.com/t...   t3_1bx6qw         NaN   \n",
       "1  I've wanted to watch this for a long time. I w...  t1_c9b2nyd   t3_1bx6qw   \n",
       "2  You strike me as the type who would appreciate...  t1_c9b30i1  t1_c9b2nyd   \n",
       "3  Yeah, I've always heard that Altman was famous...  t1_c9b6sj0  t1_c9b30i1   \n",
       "4  Alright guys, little background about myself. ...    t3_omv7p         NaN   \n",
       "\n",
       "   is_self_post majority_link majority_type  post_depth         subreddit  \\\n",
       "0          True          none  announcement         0.0  100movies365days   \n",
       "1          True     t3_1bx6qw   elaboration         1.0  100movies365days   \n",
       "2          True    t1_c9b2nyd   elaboration         2.0  100movies365days   \n",
       "3          True    t1_c9b30i1   elaboration         3.0  100movies365days   \n",
       "4          True          none  announcement         0.0           100sets   \n",
       "\n",
       "  thread_author                             thread_title  \\\n",
       "0        DTX120                  DTX120: #87 - Nashville   \n",
       "1        DTX120                  DTX120: #87 - Nashville   \n",
       "2        DTX120                  DTX120: #87 - Nashville   \n",
       "3        DTX120                  DTX120: #87 - Nashville   \n",
       "4      Keatonus  Male, 23 years old. Going for 100 sets!   \n",
       "\n",
       "                                                 url  n_sentences  n_words  \n",
       "0  https://www.reddit.com/r/100movies365days/comm...           29      499  \n",
       "1                                                NaN            2       22  \n",
       "2                                                NaN            4       82  \n",
       "3                                                NaN            2       26  \n",
       "4  https://www.reddit.com/r/100sets/comments/omv7...            5      115  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_number = lambda b : len(nltk.tokenize.word_tokenize(b))\n",
    "\n",
    "df_sample['n_words'] = df_sample['body'].map(words_number)\n",
    "\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of characters\n",
    "\n",
    "Here we create a tokenizer from a RegEx to remove punctuaions and whitespaces, so we can count only characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "289"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "\n",
    "words = tokenizer.tokenize(df_sample['body'].iloc[42])\n",
    "\n",
    "sum(map(len, words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bruno/anaconda3/envs/coarse-discourse-validation/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotations</th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>id</th>\n",
       "      <th>in_reply_to</th>\n",
       "      <th>is_self_post</th>\n",
       "      <th>majority_link</th>\n",
       "      <th>majority_type</th>\n",
       "      <th>post_depth</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>thread_author</th>\n",
       "      <th>thread_title</th>\n",
       "      <th>url</th>\n",
       "      <th>n_sentences</th>\n",
       "      <th>n_words</th>\n",
       "      <th>n_char</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'link_to_post': 'none', 'main_type': 'announ...</td>\n",
       "      <td>DTX120</td>\n",
       "      <td>4/7/13  \\n\\n7/27/12  \\n\\nhttp://www.imdb.com/t...</td>\n",
       "      <td>t3_1bx6qw</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>announcement</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100movies365days</td>\n",
       "      <td>DTX120</td>\n",
       "      <td>DTX120: #87 - Nashville</td>\n",
       "      <td>https://www.reddit.com/r/100movies365days/comm...</td>\n",
       "      <td>29</td>\n",
       "      <td>499</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'link_to_post': 't3_1bx6qw', 'main_type': 'a...</td>\n",
       "      <td>mcgrewf10</td>\n",
       "      <td>I've wanted to watch this for a long time. I w...</td>\n",
       "      <td>t1_c9b2nyd</td>\n",
       "      <td>t3_1bx6qw</td>\n",
       "      <td>True</td>\n",
       "      <td>t3_1bx6qw</td>\n",
       "      <td>elaboration</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100movies365days</td>\n",
       "      <td>DTX120</td>\n",
       "      <td>DTX120: #87 - Nashville</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{'link_to_post': 't1_c9b2nyd', 'main_type': '...</td>\n",
       "      <td>DTX120</td>\n",
       "      <td>You strike me as the type who would appreciate...</td>\n",
       "      <td>t1_c9b30i1</td>\n",
       "      <td>t1_c9b2nyd</td>\n",
       "      <td>True</td>\n",
       "      <td>t1_c9b2nyd</td>\n",
       "      <td>elaboration</td>\n",
       "      <td>2.0</td>\n",
       "      <td>100movies365days</td>\n",
       "      <td>DTX120</td>\n",
       "      <td>DTX120: #87 - Nashville</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>82</td>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{'link_to_post': 't1_c9b30i1', 'main_type': '...</td>\n",
       "      <td>mcgrewf10</td>\n",
       "      <td>Yeah, I've always heard that Altman was famous...</td>\n",
       "      <td>t1_c9b6sj0</td>\n",
       "      <td>t1_c9b30i1</td>\n",
       "      <td>True</td>\n",
       "      <td>t1_c9b30i1</td>\n",
       "      <td>elaboration</td>\n",
       "      <td>3.0</td>\n",
       "      <td>100movies365days</td>\n",
       "      <td>DTX120</td>\n",
       "      <td>DTX120: #87 - Nashville</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{'link_to_post': 'none', 'main_type': 'announ...</td>\n",
       "      <td>Keatonus</td>\n",
       "      <td>Alright guys, little background about myself. ...</td>\n",
       "      <td>t3_omv7p</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>announcement</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100sets</td>\n",
       "      <td>Keatonus</td>\n",
       "      <td>Male, 23 years old. Going for 100 sets!</td>\n",
       "      <td>https://www.reddit.com/r/100sets/comments/omv7...</td>\n",
       "      <td>5</td>\n",
       "      <td>115</td>\n",
       "      <td>434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         annotations     author  \\\n",
       "0  [{'link_to_post': 'none', 'main_type': 'announ...     DTX120   \n",
       "1  [{'link_to_post': 't3_1bx6qw', 'main_type': 'a...  mcgrewf10   \n",
       "2  [{'link_to_post': 't1_c9b2nyd', 'main_type': '...     DTX120   \n",
       "3  [{'link_to_post': 't1_c9b30i1', 'main_type': '...  mcgrewf10   \n",
       "4  [{'link_to_post': 'none', 'main_type': 'announ...   Keatonus   \n",
       "\n",
       "                                                body          id in_reply_to  \\\n",
       "0  4/7/13  \\n\\n7/27/12  \\n\\nhttp://www.imdb.com/t...   t3_1bx6qw         NaN   \n",
       "1  I've wanted to watch this for a long time. I w...  t1_c9b2nyd   t3_1bx6qw   \n",
       "2  You strike me as the type who would appreciate...  t1_c9b30i1  t1_c9b2nyd   \n",
       "3  Yeah, I've always heard that Altman was famous...  t1_c9b6sj0  t1_c9b30i1   \n",
       "4  Alright guys, little background about myself. ...    t3_omv7p         NaN   \n",
       "\n",
       "   is_self_post majority_link majority_type  post_depth         subreddit  \\\n",
       "0          True          none  announcement         0.0  100movies365days   \n",
       "1          True     t3_1bx6qw   elaboration         1.0  100movies365days   \n",
       "2          True    t1_c9b2nyd   elaboration         2.0  100movies365days   \n",
       "3          True    t1_c9b30i1   elaboration         3.0  100movies365days   \n",
       "4          True          none  announcement         0.0           100sets   \n",
       "\n",
       "  thread_author                             thread_title  \\\n",
       "0        DTX120                  DTX120: #87 - Nashville   \n",
       "1        DTX120                  DTX120: #87 - Nashville   \n",
       "2        DTX120                  DTX120: #87 - Nashville   \n",
       "3        DTX120                  DTX120: #87 - Nashville   \n",
       "4      Keatonus  Male, 23 years old. Going for 100 sets!   \n",
       "\n",
       "                                                 url  n_sentences  n_words  \\\n",
       "0  https://www.reddit.com/r/100movies365days/comm...           29      499   \n",
       "1                                                NaN            2       22   \n",
       "2                                                NaN            4       82   \n",
       "3                                                NaN            2       26   \n",
       "4  https://www.reddit.com/r/100sets/comments/omv7...            5      115   \n",
       "\n",
       "   n_char  \n",
       "0    2009  \n",
       "1      74  \n",
       "2     339  \n",
       "3      84  \n",
       "4     434  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_number = lambda b : sum(map(len, tokenizer.tokenize(b)))\n",
    "\n",
    "df_sample['n_char'] = df_sample['body'].map(char_number)\n",
    "\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parents number of sentences, words and characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bruno/anaconda3/envs/coarse-discourse-validation/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/bruno/anaconda3/envs/coarse-discourse-validation/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/home/bruno/anaconda3/envs/coarse-discourse-validation/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/bruno/anaconda3/envs/coarse-discourse-validation/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotations</th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>id</th>\n",
       "      <th>in_reply_to</th>\n",
       "      <th>is_self_post</th>\n",
       "      <th>majority_link</th>\n",
       "      <th>majority_type</th>\n",
       "      <th>post_depth</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>thread_author</th>\n",
       "      <th>thread_title</th>\n",
       "      <th>url</th>\n",
       "      <th>n_sentences</th>\n",
       "      <th>n_words</th>\n",
       "      <th>n_char</th>\n",
       "      <th>parent_n_sentences</th>\n",
       "      <th>parent_n_words</th>\n",
       "      <th>parent_n_char</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'link_to_post': 'none', 'main_type': 'announ...</td>\n",
       "      <td>DTX120</td>\n",
       "      <td>4/7/13  \\n\\n7/27/12  \\n\\nhttp://www.imdb.com/t...</td>\n",
       "      <td>t3_1bx6qw</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>announcement</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100movies365days</td>\n",
       "      <td>DTX120</td>\n",
       "      <td>DTX120: #87 - Nashville</td>\n",
       "      <td>https://www.reddit.com/r/100movies365days/comm...</td>\n",
       "      <td>29</td>\n",
       "      <td>499</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'link_to_post': 't3_1bx6qw', 'main_type': 'a...</td>\n",
       "      <td>mcgrewf10</td>\n",
       "      <td>I've wanted to watch this for a long time. I w...</td>\n",
       "      <td>t1_c9b2nyd</td>\n",
       "      <td>t3_1bx6qw</td>\n",
       "      <td>True</td>\n",
       "      <td>t3_1bx6qw</td>\n",
       "      <td>elaboration</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100movies365days</td>\n",
       "      <td>DTX120</td>\n",
       "      <td>DTX120: #87 - Nashville</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>74</td>\n",
       "      <td>29</td>\n",
       "      <td>499</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{'link_to_post': 't1_c9b2nyd', 'main_type': '...</td>\n",
       "      <td>DTX120</td>\n",
       "      <td>You strike me as the type who would appreciate...</td>\n",
       "      <td>t1_c9b30i1</td>\n",
       "      <td>t1_c9b2nyd</td>\n",
       "      <td>True</td>\n",
       "      <td>t1_c9b2nyd</td>\n",
       "      <td>elaboration</td>\n",
       "      <td>2.0</td>\n",
       "      <td>100movies365days</td>\n",
       "      <td>DTX120</td>\n",
       "      <td>DTX120: #87 - Nashville</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>82</td>\n",
       "      <td>339</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{'link_to_post': 't1_c9b30i1', 'main_type': '...</td>\n",
       "      <td>mcgrewf10</td>\n",
       "      <td>Yeah, I've always heard that Altman was famous...</td>\n",
       "      <td>t1_c9b6sj0</td>\n",
       "      <td>t1_c9b30i1</td>\n",
       "      <td>True</td>\n",
       "      <td>t1_c9b30i1</td>\n",
       "      <td>elaboration</td>\n",
       "      <td>3.0</td>\n",
       "      <td>100movies365days</td>\n",
       "      <td>DTX120</td>\n",
       "      <td>DTX120: #87 - Nashville</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>84</td>\n",
       "      <td>4</td>\n",
       "      <td>82</td>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{'link_to_post': 'none', 'main_type': 'announ...</td>\n",
       "      <td>Keatonus</td>\n",
       "      <td>Alright guys, little background about myself. ...</td>\n",
       "      <td>t3_omv7p</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>announcement</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100sets</td>\n",
       "      <td>Keatonus</td>\n",
       "      <td>Male, 23 years old. Going for 100 sets!</td>\n",
       "      <td>https://www.reddit.com/r/100sets/comments/omv7...</td>\n",
       "      <td>5</td>\n",
       "      <td>115</td>\n",
       "      <td>434</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         annotations     author  \\\n",
       "0  [{'link_to_post': 'none', 'main_type': 'announ...     DTX120   \n",
       "1  [{'link_to_post': 't3_1bx6qw', 'main_type': 'a...  mcgrewf10   \n",
       "2  [{'link_to_post': 't1_c9b2nyd', 'main_type': '...     DTX120   \n",
       "3  [{'link_to_post': 't1_c9b30i1', 'main_type': '...  mcgrewf10   \n",
       "4  [{'link_to_post': 'none', 'main_type': 'announ...   Keatonus   \n",
       "\n",
       "                                                body          id in_reply_to  \\\n",
       "0  4/7/13  \\n\\n7/27/12  \\n\\nhttp://www.imdb.com/t...   t3_1bx6qw         NaN   \n",
       "1  I've wanted to watch this for a long time. I w...  t1_c9b2nyd   t3_1bx6qw   \n",
       "2  You strike me as the type who would appreciate...  t1_c9b30i1  t1_c9b2nyd   \n",
       "3  Yeah, I've always heard that Altman was famous...  t1_c9b6sj0  t1_c9b30i1   \n",
       "4  Alright guys, little background about myself. ...    t3_omv7p         NaN   \n",
       "\n",
       "   is_self_post majority_link majority_type  post_depth         subreddit  \\\n",
       "0          True          none  announcement         0.0  100movies365days   \n",
       "1          True     t3_1bx6qw   elaboration         1.0  100movies365days   \n",
       "2          True    t1_c9b2nyd   elaboration         2.0  100movies365days   \n",
       "3          True    t1_c9b30i1   elaboration         3.0  100movies365days   \n",
       "4          True          none  announcement         0.0           100sets   \n",
       "\n",
       "  thread_author                             thread_title  \\\n",
       "0        DTX120                  DTX120: #87 - Nashville   \n",
       "1        DTX120                  DTX120: #87 - Nashville   \n",
       "2        DTX120                  DTX120: #87 - Nashville   \n",
       "3        DTX120                  DTX120: #87 - Nashville   \n",
       "4      Keatonus  Male, 23 years old. Going for 100 sets!   \n",
       "\n",
       "                                                 url  n_sentences  n_words  \\\n",
       "0  https://www.reddit.com/r/100movies365days/comm...           29      499   \n",
       "1                                                NaN            2       22   \n",
       "2                                                NaN            4       82   \n",
       "3                                                NaN            2       26   \n",
       "4  https://www.reddit.com/r/100sets/comments/omv7...            5      115   \n",
       "\n",
       "   n_char  parent_n_sentences  parent_n_words  parent_n_char  \n",
       "0    2009                   0               0              0  \n",
       "1      74                  29             499           2009  \n",
       "2     339                   2              22             74  \n",
       "3      84                   4              82            339  \n",
       "4     434                   0               0              0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample['parent_n_sentences'] = 0\n",
    "df_sample['parent_n_words'] = 0\n",
    "df_sample['parent_n_char'] = 0\n",
    "\n",
    "for i, row in df_sample.iterrows():\n",
    "    try:\n",
    "        parent = df_sample[(df_sample['thread_title'] == row['thread_title'])][df_sample['id'] == row['in_reply_to']].iloc[0]\n",
    "        df_sample.at[i, 'parent_n_sentences'] = parent['n_sentences']\n",
    "        df_sample.at[i, 'parent_n_words'] = parent['n_words']\n",
    "        df_sample.at[i, 'parent_n_char'] = parent['n_char']\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author\n",
    "\n",
    "> We collect features about the author of the comment, including a binary feature for whether the current commenter is also the commenter of the initial post and a binary feature for whether the current commenter is the same as the parent commenter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bruno/anaconda3/envs/coarse-discourse-validation/lib/python3.7/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotations</th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>id</th>\n",
       "      <th>in_reply_to</th>\n",
       "      <th>is_self_post</th>\n",
       "      <th>majority_link</th>\n",
       "      <th>majority_type</th>\n",
       "      <th>post_depth</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>thread_author</th>\n",
       "      <th>thread_title</th>\n",
       "      <th>url</th>\n",
       "      <th>n_sentences</th>\n",
       "      <th>n_words</th>\n",
       "      <th>n_char</th>\n",
       "      <th>parent_n_sentences</th>\n",
       "      <th>parent_n_words</th>\n",
       "      <th>parent_n_char</th>\n",
       "      <th>is_thread_author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'link_to_post': 'none', 'main_type': 'announ...</td>\n",
       "      <td>DTX120</td>\n",
       "      <td>4/7/13  \\n\\n7/27/12  \\n\\nhttp://www.imdb.com/t...</td>\n",
       "      <td>t3_1bx6qw</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>announcement</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100movies365days</td>\n",
       "      <td>DTX120</td>\n",
       "      <td>DTX120: #87 - Nashville</td>\n",
       "      <td>https://www.reddit.com/r/100movies365days/comm...</td>\n",
       "      <td>29</td>\n",
       "      <td>499</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'link_to_post': 't3_1bx6qw', 'main_type': 'a...</td>\n",
       "      <td>mcgrewf10</td>\n",
       "      <td>I've wanted to watch this for a long time. I w...</td>\n",
       "      <td>t1_c9b2nyd</td>\n",
       "      <td>t3_1bx6qw</td>\n",
       "      <td>True</td>\n",
       "      <td>t3_1bx6qw</td>\n",
       "      <td>elaboration</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100movies365days</td>\n",
       "      <td>DTX120</td>\n",
       "      <td>DTX120: #87 - Nashville</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>74</td>\n",
       "      <td>29</td>\n",
       "      <td>499</td>\n",
       "      <td>2009</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{'link_to_post': 't1_c9b2nyd', 'main_type': '...</td>\n",
       "      <td>DTX120</td>\n",
       "      <td>You strike me as the type who would appreciate...</td>\n",
       "      <td>t1_c9b30i1</td>\n",
       "      <td>t1_c9b2nyd</td>\n",
       "      <td>True</td>\n",
       "      <td>t1_c9b2nyd</td>\n",
       "      <td>elaboration</td>\n",
       "      <td>2.0</td>\n",
       "      <td>100movies365days</td>\n",
       "      <td>DTX120</td>\n",
       "      <td>DTX120: #87 - Nashville</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>82</td>\n",
       "      <td>339</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>74</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{'link_to_post': 't1_c9b30i1', 'main_type': '...</td>\n",
       "      <td>mcgrewf10</td>\n",
       "      <td>Yeah, I've always heard that Altman was famous...</td>\n",
       "      <td>t1_c9b6sj0</td>\n",
       "      <td>t1_c9b30i1</td>\n",
       "      <td>True</td>\n",
       "      <td>t1_c9b30i1</td>\n",
       "      <td>elaboration</td>\n",
       "      <td>3.0</td>\n",
       "      <td>100movies365days</td>\n",
       "      <td>DTX120</td>\n",
       "      <td>DTX120: #87 - Nashville</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>84</td>\n",
       "      <td>4</td>\n",
       "      <td>82</td>\n",
       "      <td>339</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{'link_to_post': 'none', 'main_type': 'announ...</td>\n",
       "      <td>Keatonus</td>\n",
       "      <td>Alright guys, little background about myself. ...</td>\n",
       "      <td>t3_omv7p</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>announcement</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100sets</td>\n",
       "      <td>Keatonus</td>\n",
       "      <td>Male, 23 years old. Going for 100 sets!</td>\n",
       "      <td>https://www.reddit.com/r/100sets/comments/omv7...</td>\n",
       "      <td>5</td>\n",
       "      <td>115</td>\n",
       "      <td>434</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         annotations     author  \\\n",
       "0  [{'link_to_post': 'none', 'main_type': 'announ...     DTX120   \n",
       "1  [{'link_to_post': 't3_1bx6qw', 'main_type': 'a...  mcgrewf10   \n",
       "2  [{'link_to_post': 't1_c9b2nyd', 'main_type': '...     DTX120   \n",
       "3  [{'link_to_post': 't1_c9b30i1', 'main_type': '...  mcgrewf10   \n",
       "4  [{'link_to_post': 'none', 'main_type': 'announ...   Keatonus   \n",
       "\n",
       "                                                body          id in_reply_to  \\\n",
       "0  4/7/13  \\n\\n7/27/12  \\n\\nhttp://www.imdb.com/t...   t3_1bx6qw         NaN   \n",
       "1  I've wanted to watch this for a long time. I w...  t1_c9b2nyd   t3_1bx6qw   \n",
       "2  You strike me as the type who would appreciate...  t1_c9b30i1  t1_c9b2nyd   \n",
       "3  Yeah, I've always heard that Altman was famous...  t1_c9b6sj0  t1_c9b30i1   \n",
       "4  Alright guys, little background about myself. ...    t3_omv7p         NaN   \n",
       "\n",
       "   is_self_post majority_link majority_type  post_depth         subreddit  \\\n",
       "0          True          none  announcement         0.0  100movies365days   \n",
       "1          True     t3_1bx6qw   elaboration         1.0  100movies365days   \n",
       "2          True    t1_c9b2nyd   elaboration         2.0  100movies365days   \n",
       "3          True    t1_c9b30i1   elaboration         3.0  100movies365days   \n",
       "4          True          none  announcement         0.0           100sets   \n",
       "\n",
       "  thread_author                             thread_title  \\\n",
       "0        DTX120                  DTX120: #87 - Nashville   \n",
       "1        DTX120                  DTX120: #87 - Nashville   \n",
       "2        DTX120                  DTX120: #87 - Nashville   \n",
       "3        DTX120                  DTX120: #87 - Nashville   \n",
       "4      Keatonus  Male, 23 years old. Going for 100 sets!   \n",
       "\n",
       "                                                 url  n_sentences  n_words  \\\n",
       "0  https://www.reddit.com/r/100movies365days/comm...           29      499   \n",
       "1                                                NaN            2       22   \n",
       "2                                                NaN            4       82   \n",
       "3                                                NaN            2       26   \n",
       "4  https://www.reddit.com/r/100sets/comments/omv7...            5      115   \n",
       "\n",
       "   n_char  parent_n_sentences  parent_n_words  parent_n_char  is_thread_author  \n",
       "0    2009                   0               0              0              True  \n",
       "1      74                  29             499           2009             False  \n",
       "2     339                   2              22             74              True  \n",
       "3      84                   4              82            339             False  \n",
       "4     434                   0               0              0              True  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_thread_author(row):\n",
    "    if row['author'] == '[deleted]' :\n",
    "        return False\n",
    "    else:\n",
    "        return row['author'] == row['thread_author']\n",
    "\n",
    "df_sample['is_thread_author'] = df_sample.apply(is_thread_author, axis=1)\n",
    "\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bruno/anaconda3/envs/coarse-discourse-validation/lib/python3.7/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotations</th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>id</th>\n",
       "      <th>in_reply_to</th>\n",
       "      <th>is_self_post</th>\n",
       "      <th>majority_link</th>\n",
       "      <th>majority_type</th>\n",
       "      <th>post_depth</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>...</th>\n",
       "      <th>thread_title</th>\n",
       "      <th>url</th>\n",
       "      <th>n_sentences</th>\n",
       "      <th>n_words</th>\n",
       "      <th>n_char</th>\n",
       "      <th>parent_n_sentences</th>\n",
       "      <th>parent_n_words</th>\n",
       "      <th>parent_n_char</th>\n",
       "      <th>is_thread_author</th>\n",
       "      <th>is_parent_author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'link_to_post': 'none', 'main_type': 'announ...</td>\n",
       "      <td>DTX120</td>\n",
       "      <td>4/7/13  \\n\\n7/27/12  \\n\\nhttp://www.imdb.com/t...</td>\n",
       "      <td>t3_1bx6qw</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>announcement</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100movies365days</td>\n",
       "      <td>...</td>\n",
       "      <td>DTX120: #87 - Nashville</td>\n",
       "      <td>https://www.reddit.com/r/100movies365days/comm...</td>\n",
       "      <td>29</td>\n",
       "      <td>499</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'link_to_post': 't3_1bx6qw', 'main_type': 'a...</td>\n",
       "      <td>mcgrewf10</td>\n",
       "      <td>I've wanted to watch this for a long time. I w...</td>\n",
       "      <td>t1_c9b2nyd</td>\n",
       "      <td>t3_1bx6qw</td>\n",
       "      <td>True</td>\n",
       "      <td>t3_1bx6qw</td>\n",
       "      <td>elaboration</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100movies365days</td>\n",
       "      <td>...</td>\n",
       "      <td>DTX120: #87 - Nashville</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>74</td>\n",
       "      <td>29</td>\n",
       "      <td>499</td>\n",
       "      <td>2009</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{'link_to_post': 't1_c9b2nyd', 'main_type': '...</td>\n",
       "      <td>DTX120</td>\n",
       "      <td>You strike me as the type who would appreciate...</td>\n",
       "      <td>t1_c9b30i1</td>\n",
       "      <td>t1_c9b2nyd</td>\n",
       "      <td>True</td>\n",
       "      <td>t1_c9b2nyd</td>\n",
       "      <td>elaboration</td>\n",
       "      <td>2.0</td>\n",
       "      <td>100movies365days</td>\n",
       "      <td>...</td>\n",
       "      <td>DTX120: #87 - Nashville</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>82</td>\n",
       "      <td>339</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>74</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{'link_to_post': 't1_c9b30i1', 'main_type': '...</td>\n",
       "      <td>mcgrewf10</td>\n",
       "      <td>Yeah, I've always heard that Altman was famous...</td>\n",
       "      <td>t1_c9b6sj0</td>\n",
       "      <td>t1_c9b30i1</td>\n",
       "      <td>True</td>\n",
       "      <td>t1_c9b30i1</td>\n",
       "      <td>elaboration</td>\n",
       "      <td>3.0</td>\n",
       "      <td>100movies365days</td>\n",
       "      <td>...</td>\n",
       "      <td>DTX120: #87 - Nashville</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>84</td>\n",
       "      <td>4</td>\n",
       "      <td>82</td>\n",
       "      <td>339</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{'link_to_post': 'none', 'main_type': 'announ...</td>\n",
       "      <td>Keatonus</td>\n",
       "      <td>Alright guys, little background about myself. ...</td>\n",
       "      <td>t3_omv7p</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>announcement</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100sets</td>\n",
       "      <td>...</td>\n",
       "      <td>Male, 23 years old. Going for 100 sets!</td>\n",
       "      <td>https://www.reddit.com/r/100sets/comments/omv7...</td>\n",
       "      <td>5</td>\n",
       "      <td>115</td>\n",
       "      <td>434</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         annotations     author  \\\n",
       "0  [{'link_to_post': 'none', 'main_type': 'announ...     DTX120   \n",
       "1  [{'link_to_post': 't3_1bx6qw', 'main_type': 'a...  mcgrewf10   \n",
       "2  [{'link_to_post': 't1_c9b2nyd', 'main_type': '...     DTX120   \n",
       "3  [{'link_to_post': 't1_c9b30i1', 'main_type': '...  mcgrewf10   \n",
       "4  [{'link_to_post': 'none', 'main_type': 'announ...   Keatonus   \n",
       "\n",
       "                                                body          id in_reply_to  \\\n",
       "0  4/7/13  \\n\\n7/27/12  \\n\\nhttp://www.imdb.com/t...   t3_1bx6qw         NaN   \n",
       "1  I've wanted to watch this for a long time. I w...  t1_c9b2nyd   t3_1bx6qw   \n",
       "2  You strike me as the type who would appreciate...  t1_c9b30i1  t1_c9b2nyd   \n",
       "3  Yeah, I've always heard that Altman was famous...  t1_c9b6sj0  t1_c9b30i1   \n",
       "4  Alright guys, little background about myself. ...    t3_omv7p         NaN   \n",
       "\n",
       "   is_self_post majority_link majority_type  post_depth         subreddit  \\\n",
       "0          True          none  announcement         0.0  100movies365days   \n",
       "1          True     t3_1bx6qw   elaboration         1.0  100movies365days   \n",
       "2          True    t1_c9b2nyd   elaboration         2.0  100movies365days   \n",
       "3          True    t1_c9b30i1   elaboration         3.0  100movies365days   \n",
       "4          True          none  announcement         0.0           100sets   \n",
       "\n",
       "         ...                                    thread_title  \\\n",
       "0        ...                         DTX120: #87 - Nashville   \n",
       "1        ...                         DTX120: #87 - Nashville   \n",
       "2        ...                         DTX120: #87 - Nashville   \n",
       "3        ...                         DTX120: #87 - Nashville   \n",
       "4        ...         Male, 23 years old. Going for 100 sets!   \n",
       "\n",
       "                                                 url n_sentences  n_words  \\\n",
       "0  https://www.reddit.com/r/100movies365days/comm...          29      499   \n",
       "1                                                NaN           2       22   \n",
       "2                                                NaN           4       82   \n",
       "3                                                NaN           2       26   \n",
       "4  https://www.reddit.com/r/100sets/comments/omv7...           5      115   \n",
       "\n",
       "   n_char  parent_n_sentences  parent_n_words  parent_n_char  \\\n",
       "0    2009                   0               0              0   \n",
       "1      74                  29             499           2009   \n",
       "2     339                   2              22             74   \n",
       "3      84                   4              82            339   \n",
       "4     434                   0               0              0   \n",
       "\n",
       "   is_thread_author  is_parent_author  \n",
       "0              True             False  \n",
       "1             False             False  \n",
       "2              True             False  \n",
       "3             False             False  \n",
       "4              True             False  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_parent_author(row):\n",
    "    try:\n",
    "        parent_author = df_sample[df_sample['id']==row['in_reply_to']].iloc[0]['author']\n",
    "        \n",
    "        return row['author'] == parent_author and row['author'] != '[deleted]'\n",
    "    except IndexError:\n",
    "        return False\n",
    "\n",
    "df_sample['is_parent_author'] = df_sample.apply(is_parent_author, axis=1)\n",
    "\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    982\n",
       "True      18\n",
       "Name: is_parent_author, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample['is_parent_author'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    756\n",
       "True     244\n",
       "Name: is_thread_author, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample['is_thread_author'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thread\n",
    "\n",
    "> We calculate features that are the same across all comments in the thread. One feature is the total number of comments in the discussion. Another is the number of unique branches in the discussion tree. We also record whether the discussion originated as a self-post or a link-post. Finally, we collect the average length of all the branches or threads of discussion in the discussion tree.\n",
    "\n",
    "Most of these may be included in the pre-processing step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Community\n",
    "\n",
    ">We have a feature naming the subreddit that the thread came from, as some subreddits have a greater porportion of some types of discourse and not others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bruno/anaconda3/envs/coarse-discourse-validation/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        100movies365days\n",
       "1        100movies365days\n",
       "2        100movies365days\n",
       "3        100movies365days\n",
       "4                 100sets\n",
       "5                 100sets\n",
       "7                 100sets\n",
       "8                 100sets\n",
       "9                 100sets\n",
       "10                100sets\n",
       "11           1200isplenty\n",
       "12           1200isplenty\n",
       "13           1200isplenty\n",
       "14           1200isplenty\n",
       "15           1200isplenty\n",
       "16           1200isplenty\n",
       "17           1200isplenty\n",
       "18           1200isplenty\n",
       "19                   1911\n",
       "20                   1911\n",
       "21                   1911\n",
       "22                   1911\n",
       "23                   1911\n",
       "24                   1911\n",
       "25                   1911\n",
       "26                   1911\n",
       "27                   1911\n",
       "28                   1911\n",
       "29                   1911\n",
       "30                   1911\n",
       "              ...        \n",
       "980         adviceanimals\n",
       "981         adviceanimals\n",
       "982         adviceanimals\n",
       "983         adviceanimals\n",
       "984         adviceanimals\n",
       "985         adviceanimals\n",
       "986         adviceanimals\n",
       "987         adviceanimals\n",
       "988         adviceanimals\n",
       "989         adviceanimals\n",
       "990         adviceanimals\n",
       "991         adviceanimals\n",
       "992         adviceanimals\n",
       "993         adviceanimals\n",
       "994         adviceanlmals\n",
       "995         adviceanlmals\n",
       "996         adviceanlmals\n",
       "997         adviceanlmals\n",
       "998     agameofthroneslcg\n",
       "999     agameofthroneslcg\n",
       "1000    agameofthroneslcg\n",
       "1001    agameofthroneslcg\n",
       "1002    agameofthroneslcg\n",
       "1003    agameofthroneslcg\n",
       "1004    agameofthroneslcg\n",
       "1005    agameofthroneslcg\n",
       "1006    agameofthroneslcg\n",
       "1007    agameofthroneslcg\n",
       "1008    agameofthroneslcg\n",
       "1009    agameofthroneslcg\n",
       "Name: subreddit, Length: 1000, dtype: category\n",
       "Categories (45, object): [100movies365days, 100sets, 1200isplenty, 1911, ..., apstudents, ark, arkone, asu]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample['subreddit'] = df_sample['subreddit'].astype('category')\n",
    "\n",
    "df_sample['subreddit']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content + Punctuation\n",
    "\n",
    ">We collect unigrams, bigrams, and trigrams from the text of the comment. If the comment has a title, in the case of the initial post, then the n-grams of the title are counted separately from the n-grams of the body. We use a word tokenizer that tokenizes punctuation instead of stripping it so that we count potentially important punctuation like question marks or exclamation points. We use TF-IDF weighting and set a minimum document frequency of 50 comments.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction\n",
    "https://www.nltk.org/book/ch06.html - 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using nltk ngram functions and Counter object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unigrams_frequency = Counter()\n",
    "bigrams_frequency = Counter()\n",
    "trigrams_frequency = Counter()\n",
    "\n",
    "for body in df_sample['body']:\n",
    "    tokens = nltk.word_tokenize(body)\n",
    "    unigrams = tokens\n",
    "    bigrams = nltk.bigrams(tokens)\n",
    "    trigrams = nltk.trigrams(tokens)\n",
    "    \n",
    "    unigrams_frequency += Counter(unigrams)\n",
    "    bigrams_frequency += Counter(bigrams)\n",
    "    trigrams_frequency += Counter(trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unigrams_frequency.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bigrams_frequency.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trigrams_frequency.most_common(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def unigram_vectorizer(body_text):\n",
    "    features = {}\n",
    "    for unigram in list(unigrams_frequency.keys()):\n",
    "        features[unigram] = False\n",
    "    for word in nltk.word_tokenize(body_text):\n",
    "        features[word] = True\n",
    "    return features\n",
    "\n",
    "def bigram_vectorizer(body_text):\n",
    "    features = {}\n",
    "    for bigram in list(bigrams_frequency.keys()):\n",
    "        features[bigram] = False\n",
    "    for bigram in nltk.bigrams(nltk.word_tokenize(body_text)):\n",
    "        features[bigram] = True\n",
    "    return features\n",
    "\n",
    "def trigram_vectorizer(body_text):\n",
    "    features = {}\n",
    "    for trigram in list(trigrams_frequency.keys()):\n",
    "        features[trigram] = False\n",
    "    for trigram in nltk.trigrams(nltk.word_tokenize(body_text)):\n",
    "        features[trigram] = True\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len(trigrams_frequency.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature = trigram_vectorizer(df_sample.iloc[42].body)\n",
    "\n",
    "len(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the feature vectors but to get the weighting correctly we would have some more work to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using sklearn methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokens_vector = df_sample['body'].apply(nltk.word_tokenize)\n",
    "\n",
    "tokens_vector.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unigram_vectorizer = CountVectorizer(stop_words=nltk.corpus.stopwords.words('english'), lowercase=False, tokenizer=nltk.word_tokenize, analyzer='word', ngram_range=(1, 1), min_df=495)\n",
    "\n",
    "X = unigram_vectorizer.fit_transform(df_sample['body'])\n",
    "\n",
    "unigram_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bigram_vectorizer = CountVectorizer(stop_words=nltk.corpus.stopwords.words('english'), lowercase=False, tokenizer=nltk.word_tokenize, analyzer='word', ngram_range=(2, 2), min_df=83)\n",
    "\n",
    "X = bigram_vectorizer.fit_transform(df_sample['body'])\n",
    "\n",
    "bigram_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trigram_vectorizer = CountVectorizer(stop_words=nltk.corpus.stopwords.words('english'), lowercase=False, tokenizer=nltk.word_tokenize, analyzer='word', ngram_range=(3, 3), min_df=20)\n",
    "\n",
    "X = trigram_vectorizer.fit_transform(df_sample['body'])\n",
    "\n",
    "trigram_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of inconsistency with the results from NLTK ngram functions, an approach based on https://gist.github.com/himzzz/4105717 will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nltk_body = df_sample['body'].apply(lambda body: (nltk.word_tokenize(body)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(word, doc):\n",
    "    return doc.count(word) / float(len(doc))\n",
    "\n",
    "def docs_containing(word, docs):\n",
    "    f = lambda doc: doc.count(word) > 0\n",
    "    return list(filter(f, docs))\n",
    "\n",
    "def idf(word, docs):\n",
    "    return math.log( len(docs) / float(len(docs_containing(word, docs))) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unigrams_frequency = Counter()\n",
    "bigrams_frequency = Counter()\n",
    "trigrams_frequency = Counter()\n",
    "TFIDF = []\n",
    "\n",
    "for raw_text in df_sample['body']:\n",
    "    tokens = nltk.word_tokenize(raw_text)\n",
    "    text = nltk.Text(tokens)\n",
    "    \n",
    "    unigrams = tokens\n",
    "    bigrams = list(nltk.bigrams(tokens))\n",
    "    trigrams = list(nltk.trigrams(tokens))\n",
    "\n",
    "    unigrams_frequency += Counter(unigrams)\n",
    "    bigrams_frequency += Counter(nltk.bigrams(tokens))\n",
    "    trigrams_frequency += Counter(nltk.trigrams(tokens))\n",
    "\n",
    "for raw_text in df_sample['body']:\n",
    "    tokens = nltk.word_tokenize(raw_text)\n",
    "    text = nltk.Text(tokens)\n",
    "    \n",
    "    unigrams = tokens\n",
    "    bigrams = list(nltk.bigrams(tokens))\n",
    "    trigrams = list(nltk.trigrams(tokens))\n",
    "\n",
    "    ngrams = []\n",
    "    ngrams.extend(unigrams)\n",
    "    ngrams.extend(bigrams)\n",
    "    ngrams.extend(trigrams)\n",
    "    \n",
    "    body = {}\n",
    "    for token in ngrams:\n",
    "        if len(token) == 1:\n",
    "            body[token] = tf(token, unigrams) * idf(token, list(unigrams_frequency.keys()))\n",
    "        elif len(token) == 2:\n",
    "            body[token] = tf(token, bigrams) * idf(token, list(bigrams_frequency.keys()))\n",
    "        elif len(token) == 3:\n",
    "            body[token] = tf(token, trigrams) * idf(token, list(trigrams_frequency.keys()))\n",
    "    \n",
    "    TFIDF.append(body)\n",
    "\n",
    "df_sample['TFIDF'] = pd.Series(TFIDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Too much headache, let's try the pandas way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bruno/anaconda3/envs/coarse-discourse-validation/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/bruno/anaconda3/envs/coarse-discourse-validation/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/home/bruno/anaconda3/envs/coarse-discourse-validation/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "df_sample['tokens'] = df_sample['body'].apply(lambda body: nltk.word_tokenize(body))\n",
    "df_sample['bigrams'] = df_sample['tokens'].apply(lambda tokens : list(nltk.bigrams(tokens)))\n",
    "df_sample['trigrams'] = df_sample['tokens'].apply(lambda tokens : list(nltk.trigrams(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams_frequency = df_sample['tokens'].apply(Counter).sum()\n",
    "bigrams_frequency = df_sample['bigrams'].apply(Counter).sum()\n",
    "trigrams_frequency = df_sample['trigrams'].apply(Counter).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bruno/anaconda3/envs/coarse-discourse-validation/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_sample['TF-IDF'] = [dict() for _ in range(len(df_sample))]\n",
    "\n",
    "for i, row in df_sample.iterrows():\n",
    "    # TODO: Change code to make feature vector already as columns from here\n",
    "    for token in row['tokens']:\n",
    "        row['TF-IDF'][token] = tf(token, row['tokens']) * idf(token, df_sample['tokens'])\n",
    "    \n",
    "    try:\n",
    "        for token in row['bigrams']:\n",
    "            row['TF-IDF'][token] = tf(token, row['bigrams']) * idf(token, df_sample['bigrams'])\n",
    "    except ZeroDivisionError:\n",
    "        print(token)\n",
    "        raise\n",
    "    \n",
    "    for token in row['trigrams']:\n",
    "        row['TF-IDF'][token] = tf(token, row['trigrams']) * idf(token, df_sample['trigrams'])\n",
    "    \n",
    "    # df_sample.at[i, 'TF-IDF'] = tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I': 0.05134503209512621,\n",
       " 'love': 0.06876960046169048,\n",
       " 'cheese': 0.10466924306872122,\n",
       " 'cake': 0.06223202954037961,\n",
       " '!': 0.030599443714247138,\n",
       " 'both': 0.031896031072033,\n",
       " 'making': 0.03725375276344465,\n",
       " 'and': 0.026725984451594336,\n",
       " 'eating': 0.049742891151912126,\n",
       " 'it': 0.03105009473256369,\n",
       " ',': 0.04734625273672136,\n",
       " 'so': 0.018380367824563554,\n",
       " \"'m\": 0.020654367226494393,\n",
       " 'sad': 0.049742891151912126,\n",
       " 'to': 0.007074436661871631,\n",
       " 'see': 0.08321929412143357,\n",
       " 'that': 0.049242945363016606,\n",
       " 'they': 0.02231476108233174,\n",
       " 'usually': 0.04062936942507897,\n",
       " 'have': 0.014589984218807224,\n",
       " 'the': 0.006802005263766061,\n",
       " 'most': 0.031009183569210907,\n",
       " 'crazy': 0.04609005234012687,\n",
       " 'calorie': 0.06223202954037961,\n",
       " 'counts': 0.055987460346145865,\n",
       " 'do': 0.015398722952399201,\n",
       " \"n't\": 0.02761219587924274,\n",
       " 'many': 0.07238528866821571,\n",
       " 'low': 0.039124377671356764,\n",
       " 'cal': 0.055987460346145865,\n",
       " 'recipes': 0.06223202954037961,\n",
       " 'for': 0.033737109816824755,\n",
       " 'them': 0.024764614374977165,\n",
       " 'especially': 0.04609005234012687,\n",
       " 'not': 0.018952560668908835,\n",
       " 'ones': 0.03670758499933262,\n",
       " 'can': 0.018380367824563554,\n",
       " 'work': 0.027739764707144524,\n",
       " 'different': 0.03438480023084524,\n",
       " 'people': 0.0287764253358363,\n",
       " '(': 0.03235617100226295,\n",
       " 'instance': 0.06223202954037961,\n",
       " 'a': 0.016241479628475757,\n",
       " 'lot': 0.02834734381341058,\n",
       " 'require': 0.05233462153436061,\n",
       " 'sugar': 0.055987460346145865,\n",
       " 'free': 0.03398433390137827,\n",
       " 'Jell-O': 0.055987460346145865,\n",
       " 'cool': 0.0424372135283416,\n",
       " 'whip': 0.05233462153436061,\n",
       " 'Neufchatel': 0.06223202954037961,\n",
       " 'cream': 0.055987460346145865,\n",
       " 'very': 0.0575528506716726,\n",
       " 'specific': 0.039845483145893125,\n",
       " 'items': 0.0424372135283416,\n",
       " 'are': 0.017396590419473536,\n",
       " 'available': 0.039124377671356764,\n",
       " 'everywhere': 0.04773258888782014,\n",
       " ')': 0.04418258703833487,\n",
       " '.': 0.004006538937490693,\n",
       " '[': 0.02596759989411701,\n",
       " 'So': 0.02834734381341058,\n",
       " 'here': 0.028998881305118924,\n",
       " 'is': 0.011277148361822382,\n",
       " 'reciped': 0.06223202954037961,\n",
       " 'made': 0.03725375276344465,\n",
       " ']': 0.025808144244572857,\n",
       " 'http': 0.025050638684117527,\n",
       " ':': 0.03030646135619704,\n",
       " '//imgur.com/a/z6VbS': 0.06223202954037961,\n",
       " \"'s\": 0.014867206368958158,\n",
       " 'delicious': 0.05233462153436061,\n",
       " 'hope': 0.038456738282584495,\n",
       " 'others': 0.04062936942507897,\n",
       " 'will': 0.022982400471104013,\n",
       " 'enjoy': 0.043498321957678394,\n",
       " 'too': 0.025808144244572857,\n",
       " ('I', 'love'): 0.0856460127571985,\n",
       " ('love', 'cheese'): 0.06279777526347396,\n",
       " ('cheese', 'cake'): 0.06279777526347396,\n",
       " ('cake', '!'): 0.06279777526347396,\n",
       " ('!', 'I'): 0.03187779906654529,\n",
       " ('love', 'both'): 0.06279777526347396,\n",
       " ('both', 'making'): 0.06279777526347396,\n",
       " ('making', 'and'): 0.06279777526347396,\n",
       " ('and', 'eating'): 0.06279777526347396,\n",
       " ('eating', 'it'): 0.06279777526347396,\n",
       " ('it', ','): 0.032505006989153935,\n",
       " (',', 'so'): 0.029262507498801823,\n",
       " ('so', 'I'): 0.03129108523802191,\n",
       " ('I', \"'m\"): 0.021023958444068607,\n",
       " (\"'m\", 'sad'): 0.06279777526347396,\n",
       " ('sad', 'to'): 0.06279777526347396,\n",
       " ('to', 'see'): 0.04020771481085579,\n",
       " ('see', 'that'): 0.056496437258383554,\n",
       " ('that', 'they'): 0.04282300637859925,\n",
       " ('they', 'usually'): 0.06279777526347396,\n",
       " ('usually', 'have'): 0.06279777526347396,\n",
       " ('have', 'the'): 0.037041290317508364,\n",
       " ('the', 'most'): 0.04389376124820274,\n",
       " ('most', 'crazy'): 0.06279777526347396,\n",
       " ('crazy', 'calorie'): 0.06279777526347396,\n",
       " ('calorie', 'counts'): 0.06279777526347396,\n",
       " ('counts', ','): 0.06279777526347396,\n",
       " (',', 'and'): 0.03948285146522985,\n",
       " ('and', 'I'): 0.054107811750142024,\n",
       " ('I', 'do'): 0.026367473579542416,\n",
       " ('do', \"n't\"): 0.02169060638120997,\n",
       " (\"n't\", 'see'): 0.05019509925329314,\n",
       " ('see', 'many'): 0.06279777526347396,\n",
       " ('many', 'low'): 0.06279777526347396,\n",
       " ('low', 'cal'): 0.06279777526347396,\n",
       " ('cal', 'recipes'): 0.06279777526347396,\n",
       " ('recipes', 'for'): 0.06279777526347396,\n",
       " ('for', 'them'): 0.04816652151407305,\n",
       " ('them', 'especially'): 0.06279777526347396,\n",
       " ('especially', 'not'): 0.06279777526347396,\n",
       " ('not', 'ones'): 0.06279777526347396,\n",
       " ('ones', 'that'): 0.06279777526347396,\n",
       " ('that', 'can'): 0.05019509925329314,\n",
       " ('can', 'work'): 0.056496437258383554,\n",
       " ('work', 'for'): 0.0465090528159462,\n",
       " ('for', 'many'): 0.056496437258383554,\n",
       " ('many', 'different'): 0.056496437258383554,\n",
       " ('different', 'people'): 0.056496437258383554,\n",
       " ('people', '('): 0.06279777526347396,\n",
       " ('(', 'for'): 0.06279777526347396,\n",
       " ('for', 'instance'): 0.06279777526347396,\n",
       " ('instance', ','): 0.06279777526347396,\n",
       " (',', 'I'): 0.021991990084090883,\n",
       " ('I', 'see'): 0.04282300637859925,\n",
       " ('see', 'a'): 0.0465090528159462,\n",
       " ('a', 'lot'): 0.029262507498801823,\n",
       " ('lot', 'that'): 0.06279777526347396,\n",
       " ('that', 'require'): 0.06279777526347396,\n",
       " ('require', 'sugar'): 0.06279777526347396,\n",
       " ('sugar', 'free'): 0.06279777526347396,\n",
       " ('free', 'Jell-O'): 0.06279777526347396,\n",
       " ('Jell-O', ','): 0.06279777526347396,\n",
       " (',', 'cool'): 0.05281039082103661,\n",
       " ('cool', 'whip'): 0.056496437258383554,\n",
       " ('whip', ','): 0.056496437258383554,\n",
       " (',', 'Neufchatel'): 0.06279777526347396,\n",
       " ('Neufchatel', 'cream'): 0.06279777526347396,\n",
       " ('cream', 'cheese'): 0.06279777526347396,\n",
       " ('cheese', ','): 0.056496437258383554,\n",
       " (',', 'very'): 0.06279777526347396,\n",
       " ('very', 'specific'): 0.056496437258383554,\n",
       " ('specific', 'items'): 0.056496437258383554,\n",
       " ('items', 'that'): 0.056496437258383554,\n",
       " ('that', 'are'): 0.04389376124820274,\n",
       " ('are', \"n't\"): 0.04020771481085579,\n",
       " (\"n't\", 'available'): 0.05281039082103661,\n",
       " ('available', 'everywhere'): 0.056496437258383554,\n",
       " ('everywhere', ')'): 0.06279777526347396,\n",
       " (')', '.'): 0.027233929759581735,\n",
       " ('.', '['): 0.05019509925329314,\n",
       " ('[', 'So'): 0.06279777526347396,\n",
       " ('So', 'here'): 0.056496437258383554,\n",
       " ('here', 'is'): 0.05019509925329314,\n",
       " ('is', 'a'): 0.03129108523802191,\n",
       " ('a', 'reciped'): 0.06279777526347396,\n",
       " ('reciped', 'I'): 0.06279777526347396,\n",
       " ('I', 'made'): 0.05019509925329314,\n",
       " ('made', '!'): 0.06279777526347396,\n",
       " ('!', ']'): 0.056496437258383554,\n",
       " (']', '('): 0.028396051318762505,\n",
       " ('(', 'http'): 0.030476429249933847,\n",
       " ('http', ':'): 0.02527837176306405,\n",
       " (':', '//imgur.com/a/z6VbS'): 0.06279777526347396,\n",
       " ('//imgur.com/a/z6VbS', ')'): 0.06279777526347396,\n",
       " (')', 'it'): 0.056496437258383554,\n",
       " ('it', \"'s\"): 0.024438614307327546,\n",
       " (\"'s\", 'very'): 0.05281039082103661,\n",
       " ('very', 'delicious'): 0.06279777526347396,\n",
       " ('delicious', ','): 0.056496437258383554,\n",
       " ('I', 'hope'): 0.04186518350898265,\n",
       " ('hope', 'others'): 0.06279777526347396,\n",
       " ('others', 'will'): 0.06279777526347396,\n",
       " ('will', 'enjoy'): 0.06279777526347396,\n",
       " ('enjoy', 'it'): 0.056496437258383554,\n",
       " ('it', 'too'): 0.04816652151407305,\n",
       " ('too', ':'): 0.06279777526347396,\n",
       " (':', ')'): 0.02903802920252572,\n",
       " ('I', 'love', 'cheese'): 0.06337390164203796,\n",
       " ('love', 'cheese', 'cake'): 0.06337390164203796,\n",
       " ('cheese', 'cake', '!'): 0.06337390164203796,\n",
       " ('cake', '!', 'I'): 0.06337390164203796,\n",
       " ('!', 'I', 'love'): 0.05701475319653387,\n",
       " ('I', 'love', 'both'): 0.06337390164203796,\n",
       " ('love', 'both', 'making'): 0.06337390164203796,\n",
       " ('both', 'making', 'and'): 0.06337390164203796,\n",
       " ('making', 'and', 'eating'): 0.06337390164203796,\n",
       " ('and', 'eating', 'it'): 0.06337390164203796,\n",
       " ('eating', 'it', ','): 0.06337390164203796,\n",
       " ('it', ',', 'so'): 0.05701475319653387,\n",
       " (',', 'so', 'I'): 0.03793730786002162,\n",
       " ('so', 'I', \"'m\"): 0.046935741373890665,\n",
       " ('I', \"'m\", 'sad'): 0.06337390164203796,\n",
       " (\"'m\", 'sad', 'to'): 0.06337390164203796,\n",
       " ('sad', 'to', 'see'): 0.06337390164203796,\n",
       " ('to', 'see', 'that'): 0.06337390164203796,\n",
       " ('see', 'that', 'they'): 0.06337390164203796,\n",
       " ('that', 'they', 'usually'): 0.06337390164203796,\n",
       " ('they', 'usually', 'have'): 0.06337390164203796,\n",
       " ('usually', 'have', 'the'): 0.06337390164203796,\n",
       " ('have', 'the', 'most'): 0.06337390164203796,\n",
       " ('the', 'most', 'crazy'): 0.06337390164203796,\n",
       " ('most', 'crazy', 'calorie'): 0.06337390164203796,\n",
       " ('crazy', 'calorie', 'counts'): 0.06337390164203796,\n",
       " ('calorie', 'counts', ','): 0.06337390164203796,\n",
       " ('counts', ',', 'and'): 0.06337390164203796,\n",
       " (',', 'and', 'I'): 0.07587461572004324,\n",
       " ('and', 'I', 'do'): 0.05065560475102978,\n",
       " ('I', 'do', \"n't\"): 0.029083354685309808,\n",
       " ('do', \"n't\", 'see'): 0.05329488981939475,\n",
       " (\"n't\", 'see', 'many'): 0.06337390164203796,\n",
       " ('see', 'many', 'low'): 0.06337390164203796,\n",
       " ('many', 'low', 'cal'): 0.06337390164203796,\n",
       " ('low', 'cal', 'recipes'): 0.06337390164203796,\n",
       " ('cal', 'recipes', 'for'): 0.06337390164203796,\n",
       " ('recipes', 'for', 'them'): 0.06337390164203796,\n",
       " ('for', 'them', 'especially'): 0.06337390164203796,\n",
       " ('them', 'especially', 'not'): 0.06337390164203796,\n",
       " ('especially', 'not', 'ones'): 0.06337390164203796,\n",
       " ('not', 'ones', 'that'): 0.06337390164203796,\n",
       " ('ones', 'that', 'can'): 0.06337390164203796,\n",
       " ('that', 'can', 'work'): 0.06337390164203796,\n",
       " ('can', 'work', 'for'): 0.06337390164203796,\n",
       " ('work', 'for', 'many'): 0.06337390164203796,\n",
       " ('for', 'many', 'different'): 0.06337390164203796,\n",
       " ('many', 'different', 'people'): 0.06337390164203796,\n",
       " ('different', 'people', '('): 0.06337390164203796,\n",
       " ('people', '(', 'for'): 0.06337390164203796,\n",
       " ('(', 'for', 'instance'): 0.06337390164203796,\n",
       " ('for', 'instance', ','): 0.06337390164203796,\n",
       " ('instance', ',', 'I'): 0.06337390164203796,\n",
       " (',', 'I', 'see'): 0.05329488981939475,\n",
       " ('I', 'see', 'a'): 0.06337390164203796,\n",
       " ('see', 'a', 'lot'): 0.06337390164203796,\n",
       " ('a', 'lot', 'that'): 0.06337390164203796,\n",
       " ('lot', 'that', 'require'): 0.06337390164203796,\n",
       " ('that', 'require', 'sugar'): 0.06337390164203796,\n",
       " ('require', 'sugar', 'free'): 0.06337390164203796,\n",
       " ('sugar', 'free', 'Jell-O'): 0.06337390164203796,\n",
       " ('free', 'Jell-O', ','): 0.06337390164203796,\n",
       " ('Jell-O', ',', 'cool'): 0.06337390164203796,\n",
       " (',', 'cool', 'whip'): 0.05701475319653387,\n",
       " ('cool', 'whip', ','): 0.06337390164203796,\n",
       " ('whip', ',', 'Neufchatel'): 0.06337390164203796,\n",
       " (',', 'Neufchatel', 'cream'): 0.06337390164203796,\n",
       " ('Neufchatel', 'cream', 'cheese'): 0.06337390164203796,\n",
       " ('cream', 'cheese', ','): 0.06337390164203796,\n",
       " ('cheese', ',', 'very'): 0.06337390164203796,\n",
       " (',', 'very', 'specific'): 0.06337390164203796,\n",
       " ('very', 'specific', 'items'): 0.05701475319653387,\n",
       " ('specific', 'items', 'that'): 0.05701475319653387,\n",
       " ('items', 'that', 'are'): 0.05701475319653387,\n",
       " ('that', 'are', \"n't\"): 0.05701475319653387,\n",
       " ('are', \"n't\", 'available'): 0.05701475319653387,\n",
       " (\"n't\", 'available', 'everywhere'): 0.05701475319653387,\n",
       " ('available', 'everywhere', ')'): 0.06337390164203796,\n",
       " ('everywhere', ')', '.'): 0.06337390164203796,\n",
       " (')', '.', '['): 0.06337390164203796,\n",
       " ('.', '[', 'So'): 0.06337390164203796,\n",
       " ('[', 'So', 'here'): 0.06337390164203796,\n",
       " ('So', 'here', 'is'): 0.05701475319653387,\n",
       " ('here', 'is', 'a'): 0.05701475319653387,\n",
       " ('is', 'a', 'reciped'): 0.06337390164203796,\n",
       " ('a', 'reciped', 'I'): 0.06337390164203796,\n",
       " ('reciped', 'I', 'made'): 0.06337390164203796,\n",
       " ('I', 'made', '!'): 0.06337390164203796,\n",
       " ('made', '!', ']'): 0.06337390164203796,\n",
       " ('!', ']', '('): 0.05701475319653387,\n",
       " (']', '(', 'http'): 0.03157815941451753,\n",
       " ('(', 'http', ':'): 0.030756029518281866,\n",
       " ('http', ':', '//imgur.com/a/z6VbS'): 0.06337390164203796,\n",
       " (':', '//imgur.com/a/z6VbS', ')'): 0.06337390164203796,\n",
       " ('//imgur.com/a/z6VbS', ')', 'it'): 0.06337390164203796,\n",
       " (')', 'it', \"'s\"): 0.06337390164203796,\n",
       " ('it', \"'s\", 'very'): 0.05329488981939475,\n",
       " (\"'s\", 'very', 'delicious'): 0.06337390164203796,\n",
       " ('very', 'delicious', ','): 0.06337390164203796,\n",
       " ('delicious', ',', 'and'): 0.06337390164203796,\n",
       " ('and', 'I', 'hope'): 0.05701475319653387,\n",
       " ('I', 'hope', 'others'): 0.06337390164203796,\n",
       " ('hope', 'others', 'will'): 0.06337390164203796,\n",
       " ('others', 'will', 'enjoy'): 0.06337390164203796,\n",
       " ('will', 'enjoy', 'it'): 0.06337390164203796,\n",
       " ('enjoy', 'it', 'too'): 0.05701475319653387,\n",
       " ('it', 'too', ':'): 0.06337390164203796,\n",
       " ('too', ':', ')'): 0.06337390164203796}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample.iloc[10]['TF-IDF']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we got TF-IDF weights, let's build the feature vector!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bruno/anaconda3/envs/coarse-discourse-validation/lib/python3.7/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n",
      "/home/bruno/anaconda3/envs/coarse-discourse-validation/lib/python3.7/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/home/bruno/anaconda3/envs/coarse-discourse-validation/lib/python3.7/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "def weighted_vectorizer(token_list, token_dict):\n",
    "    feature = []\n",
    "    \n",
    "    for token in token_list:\n",
    "        try:\n",
    "            feature.append(token_dict[token])\n",
    "        except KeyError:\n",
    "            feature.append(0)\n",
    "    \n",
    "    return feature\n",
    "\n",
    "\n",
    "df_sample['unigrams_feature'] = None\n",
    "df_sample['bigrams_feature'] = None\n",
    "df_sample['trigrams_feature'] = None\n",
    "\n",
    "top_unigrams = [tk for tk, freq in unigrams_frequency.most_common() if freq >= 50]\n",
    "top_bigrams = [tk for tk, freq in bigrams_frequency.most_common() if freq >= 50]\n",
    "top_trigrams = [tk for tk, freq in trigrams_frequency.most_common() if freq >= 50]\n",
    "\n",
    "for i, row in df_sample.iterrows():\n",
    "    df_sample.at[i, 'unigrams_feature'] = weighted_vectorizer(top_unigrams, row['TF-IDF'])\n",
    "    df_sample.at[i, 'bigrams_feature'] = weighted_vectorizer(top_bigrams, row['TF-IDF'])\n",
    "    df_sample.at[i, 'trigrams_feature'] = weighted_vectorizer(top_trigrams, row['TF-IDF'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bruno/anaconda3/envs/coarse-discourse-validation/lib/python3.7/site-packages/pandas/core/frame.py:3697: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotations</th>\n",
       "      <th>author</th>\n",
       "      <th>body</th>\n",
       "      <th>id</th>\n",
       "      <th>in_reply_to</th>\n",
       "      <th>is_self_post</th>\n",
       "      <th>majority_link</th>\n",
       "      <th>majority_type</th>\n",
       "      <th>post_depth</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>...</th>\n",
       "      <th>n_words</th>\n",
       "      <th>n_char</th>\n",
       "      <th>parent_n_sentences</th>\n",
       "      <th>parent_n_words</th>\n",
       "      <th>parent_n_char</th>\n",
       "      <th>is_thread_author</th>\n",
       "      <th>is_parent_author</th>\n",
       "      <th>unigrams_feature</th>\n",
       "      <th>bigrams_feature</th>\n",
       "      <th>trigrams_feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'link_to_post': 'none', 'main_type': 'announ...</td>\n",
       "      <td>DTX120</td>\n",
       "      <td>4/7/13  \\n\\n7/27/12  \\n\\nhttp://www.imdb.com/t...</td>\n",
       "      <td>t3_1bx6qw</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>announcement</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100movies365days</td>\n",
       "      <td>...</td>\n",
       "      <td>499</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.02495455514573362, 0.019559302232776677, 0....</td>\n",
       "      <td>[0.017105909557282546, 0.009287692485331513, 0...</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'link_to_post': 't3_1bx6qw', 'main_type': 'a...</td>\n",
       "      <td>mcgrewf10</td>\n",
       "      <td>I've wanted to watch this for a long time. I w...</td>\n",
       "      <td>t1_c9b2nyd</td>\n",
       "      <td>t3_1bx6qw</td>\n",
       "      <td>True</td>\n",
       "      <td>t3_1bx6qw</td>\n",
       "      <td>elaboration</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100movies365days</td>\n",
       "      <td>...</td>\n",
       "      <td>22</td>\n",
       "      <td>74</td>\n",
       "      <td>29</td>\n",
       "      <td>499</td>\n",
       "      <td>2009</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.04042962018740609, 0, 0.07401686444881832, ...</td>\n",
       "      <td>[0.08113088532882579, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{'link_to_post': 't1_c9b2nyd', 'main_type': '...</td>\n",
       "      <td>DTX120</td>\n",
       "      <td>You strike me as the type who would appreciate...</td>\n",
       "      <td>t1_c9b30i1</td>\n",
       "      <td>t1_c9b2nyd</td>\n",
       "      <td>True</td>\n",
       "      <td>t1_c9b2nyd</td>\n",
       "      <td>elaboration</td>\n",
       "      <td>2.0</td>\n",
       "      <td>100movies365days</td>\n",
       "      <td>...</td>\n",
       "      <td>82</td>\n",
       "      <td>339</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>74</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.021693942539583757, 0.01831161691211175, 0....</td>\n",
       "      <td>[0.02103393323339928, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{'link_to_post': 't1_c9b30i1', 'main_type': '...</td>\n",
       "      <td>mcgrewf10</td>\n",
       "      <td>Yeah, I've always heard that Altman was famous...</td>\n",
       "      <td>t1_c9b6sj0</td>\n",
       "      <td>t1_c9b30i1</td>\n",
       "      <td>True</td>\n",
       "      <td>t1_c9b30i1</td>\n",
       "      <td>elaboration</td>\n",
       "      <td>3.0</td>\n",
       "      <td>100movies365days</td>\n",
       "      <td>...</td>\n",
       "      <td>26</td>\n",
       "      <td>84</td>\n",
       "      <td>4</td>\n",
       "      <td>82</td>\n",
       "      <td>339</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.03420967862011285, 0.08662803385345173, 0.0...</td>\n",
       "      <td>[0, 0, 0, 0, 0.09676475636999989, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{'link_to_post': 'none', 'main_type': 'announ...</td>\n",
       "      <td>Keatonus</td>\n",
       "      <td>Alright guys, little background about myself. ...</td>\n",
       "      <td>t3_omv7p</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>none</td>\n",
       "      <td>announcement</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100sets</td>\n",
       "      <td>...</td>\n",
       "      <td>115</td>\n",
       "      <td>434</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>[0.019335905307020302, 0.0522279160623709, 0.0...</td>\n",
       "      <td>[0.01494516308688896, 0.020286275691645147, 0....</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         annotations     author  \\\n",
       "0  [{'link_to_post': 'none', 'main_type': 'announ...     DTX120   \n",
       "1  [{'link_to_post': 't3_1bx6qw', 'main_type': 'a...  mcgrewf10   \n",
       "2  [{'link_to_post': 't1_c9b2nyd', 'main_type': '...     DTX120   \n",
       "3  [{'link_to_post': 't1_c9b30i1', 'main_type': '...  mcgrewf10   \n",
       "4  [{'link_to_post': 'none', 'main_type': 'announ...   Keatonus   \n",
       "\n",
       "                                                body          id in_reply_to  \\\n",
       "0  4/7/13  \\n\\n7/27/12  \\n\\nhttp://www.imdb.com/t...   t3_1bx6qw         NaN   \n",
       "1  I've wanted to watch this for a long time. I w...  t1_c9b2nyd   t3_1bx6qw   \n",
       "2  You strike me as the type who would appreciate...  t1_c9b30i1  t1_c9b2nyd   \n",
       "3  Yeah, I've always heard that Altman was famous...  t1_c9b6sj0  t1_c9b30i1   \n",
       "4  Alright guys, little background about myself. ...    t3_omv7p         NaN   \n",
       "\n",
       "   is_self_post majority_link majority_type  post_depth         subreddit  \\\n",
       "0          True          none  announcement         0.0  100movies365days   \n",
       "1          True     t3_1bx6qw   elaboration         1.0  100movies365days   \n",
       "2          True    t1_c9b2nyd   elaboration         2.0  100movies365days   \n",
       "3          True    t1_c9b30i1   elaboration         3.0  100movies365days   \n",
       "4          True          none  announcement         0.0           100sets   \n",
       "\n",
       "         ...        n_words n_char parent_n_sentences  parent_n_words  \\\n",
       "0        ...            499   2009                  0               0   \n",
       "1        ...             22     74                 29             499   \n",
       "2        ...             82    339                  2              22   \n",
       "3        ...             26     84                  4              82   \n",
       "4        ...            115    434                  0               0   \n",
       "\n",
       "   parent_n_char  is_thread_author  is_parent_author  \\\n",
       "0              0              True             False   \n",
       "1           2009             False             False   \n",
       "2             74              True             False   \n",
       "3            339             False             False   \n",
       "4              0              True             False   \n",
       "\n",
       "                                    unigrams_feature  \\\n",
       "0  [0.02495455514573362, 0.019559302232776677, 0....   \n",
       "1  [0.04042962018740609, 0, 0.07401686444881832, ...   \n",
       "2  [0.021693942539583757, 0.01831161691211175, 0....   \n",
       "3  [0.03420967862011285, 0.08662803385345173, 0.0...   \n",
       "4  [0.019335905307020302, 0.0522279160623709, 0.0...   \n",
       "\n",
       "                                     bigrams_feature  trigrams_feature  \n",
       "0  [0.017105909557282546, 0.009287692485331513, 0...         [0, 0, 0]  \n",
       "1  [0.08113088532882579, 0, 0, 0, 0, 0, 0, 0, 0, ...         [0, 0, 0]  \n",
       "2  [0.02103393323339928, 0, 0, 0, 0, 0, 0, 0, 0, ...         [0, 0, 0]  \n",
       "3  [0, 0, 0, 0, 0.09676475636999989, 0, 0, 0, 0, ...         [0, 0, 0]  \n",
       "4  [0.01494516308688896, 0.020286275691645147, 0....         [0, 0, 0]  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample.drop(columns=['tokens', 'bigrams', 'trigrams', 'TF-IDF'], inplace=True)\n",
    "\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bruno/anaconda3/envs/coarse-discourse-validation/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/bruno/anaconda3/envs/coarse-discourse-validation/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/home/bruno/anaconda3/envs/coarse-discourse-validation/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "titles_corpus = pd.Series(list(set(df_sample['thread_title'])))\n",
    "\n",
    "df_sample['title_tokens'] = df_sample['thread_title'].apply(lambda title: nltk.word_tokenize(title))\n",
    "df_sample['title_bigrams'] = df_sample['title_tokens'].apply(lambda tokens : list(nltk.bigrams(tokens)))\n",
    "df_sample['title_trigrams'] = df_sample['title_tokens'].apply(lambda tokens : list(nltk.trigrams(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get unique values, excluding comments without title\n",
    "titles_tokens = pd.Series(list(set(df_sample['thread_title']))).apply(lambda title: nltk.word_tokenize(title))\n",
    "titles_bigrams = titles_tokens.apply(lambda tokens : list(nltk.bigrams(tokens)))\n",
    "titles_trigrams = titles_tokens.apply(lambda tokens : list(nltk.trigrams(tokens)))\n",
    "\n",
    "unigrams_frequency = titles_tokens.apply(Counter).sum()\n",
    "bigrams_frequency = titles_bigrams.apply(Counter).sum()\n",
    "trigrams_frequency = titles_trigrams.apply(Counter).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('?', 45),\n",
       " ('.', 31),\n",
       " ('I', 29),\n",
       " ('to', 27),\n",
       " ('the', 20),\n",
       " ('a', 18),\n",
       " ('for', 15),\n",
       " (\"'s\", 11),\n",
       " (',', 11),\n",
       " ('!', 8)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigrams_frequency.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are no unigrams with a frequency higher than that defined as the lower limit by the paper, we can assume that the title ngrams will be counted with the body. Since there are few titles (less impact) and we are not sure about how to take its influence into account, this **won't be implemented for now**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (coarse-discourse-validation)",
   "language": "python",
   "name": "coarse-discourse-validation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
